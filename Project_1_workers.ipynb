{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "901e960d-2cab-4761-9621-3ba9fc7d53d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/spark\n",
      "/home/ubuntu/.local/bin/pyspark\n",
      "/usr/local/spark/python:/python:\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.sql.functions import col, expr, translate,  trim, lower, regexp_replace\n",
    "from pyspark.ml.feature import Tokenizer, NGram\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "import os\n",
    "print(os.environ.get('SPARK_HOME'))\n",
    "print(os.environ.get('PYSPARK_HOME'))\n",
    "print(os.environ.get('PYTHONPATH'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28ba9d79-1b43-40ec-ad61-7fcc2a99c0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "import unicodedata\n",
    "from pyspark import SparkContext,SparkConf,SQLContext\n",
    "from pyspark.sql import Row,SparkSession,HiveContext\n",
    "from pyspark.sql.functions import col,size,explode,split\n",
    "from pyspark.sql.types import StringType,IntegerType,ArrayType\n",
    "from pyspark.sql.functions import udf, array, length\n",
    "from bs4 import BeautifulSoup\n",
    "import mistune\n",
    "import re\n",
    "#from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97453b82-11f7-4c75-b1ab-84c570c18452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/spark\n",
      "/usr/local/spark/python/pyspark/__init__.py\n"
     ]
    }
   ],
   "source": [
    "print(os.environ.get('SPARK_HOME'))  \n",
    "print(pyspark.__file__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0314a79e-c809-4857-8d94-3d3b912d69f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark/jars/spark-unsafe_2.12-3.0.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "24/03/15 08:58:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/15 08:58:38 WARN ExecutorAllocationManager: Dynamic allocation without a shuffle service is an experimental feature.\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf() \\\n",
    "    .setAppName(\"Group1_2 Project\") \\\n",
    "    .set(\"spark.hadoop.dfs.namenode\", \"hdfs://grouop2master:9866\") \\\n",
    "    .set(\"spark.sql.warehouse.dir\", \"hdfs://grouop2master:9866/user/hive/warehouse\")\\\n",
    "    .set(\"spark.executor.memory\", \"4G\")\\\n",
    "    .set(\"spark.executor.cores\", 2)\\\n",
    "    .set(\"spark.executor.instances\", 1)\\\n",
    "    .set(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "    .set(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    "    .set(\"spark.shuffle.service.enabled\", False)\n",
    "#    .setMaster(\"yarn\") \\\n",
    "#    .set(\"spark.submit.deployMode\", \"client\") \\\n",
    "#    .set(\"spark.yarn.am.memory\", \"1g\") \\\n",
    "#    .set(\"spark.executor.memory\", \"2g\") \\\n",
    "#    .set(\"spark.executor.instances\", \"2\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf)\\\n",
    "    .master(\"spark://192.168.2.247:7077\")\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c949340c-6360-488b-9807-2bd16de00fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group1_2 Project\n"
     ]
    }
   ],
   "source": [
    "if spark is not None:\n",
    "    print(spark.sparkContext.appName)  # Print the application name if it exists\n",
    "else:\n",
    "    print(\"No active SparkContext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "667d3a55-d5b4-4f87-b60f-709f8e4168e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+--------------------+-----------+-------+--------------------+--------------------+------------+--------------------+-----------+--------+\n",
      "|            author|                body|             content|content_len|     id|      normalizedBody|           subreddit|subreddit_id|             summary|summary_len|   title|\n",
      "+------------------+--------------------+--------------------+-----------+-------+--------------------+--------------------+------------+--------------------+-----------+--------+\n",
      "|  raysofdarkmatter|I think it should...|I think it should...|        178|c69al3r|I think it should...|                math|    t5_2qh0n|Shifting seasonal...|          8|    null|\n",
      "|           Stork13|Art is about the ...|Art is about the ...|        148|c6a9nxd|Art is about the ...|               funny|    t5_2qh33|Personal opinions...|          4|    null|\n",
      "|     Cloud_dreamer|Ask me what I thi...|Ask me what I thi...|         76|c6acx4l|Ask me what I thi...|         Borderlands|    t5_2r8cd|insults and slack...|         73|    null|\n",
      "|     NightlyReaper|In Mechwarrior On...|In Mechwarrior On...|        213|c8onqew|In Mechwarrior On...|            gamingpc|    t5_2sq2y|Yes, Joysticks in...|         19|    null|\n",
      "|    NuffZetPand0ra|You are talking a...|You are talking a...|        404|c6acxvc|You are talking a...|              Diablo|    t5_2qore|Class only items ...|          7|D2 help?|\n",
      "|beatlecreedcabaret|All but one of my...|All but one of my...|        130|c6ahuc4|All but one of my...|   RedditLaqueristas|    t5_2se5q|      OPI Nail Envy!|          3|    null|\n",
      "|      nobodysdiary|I could give a sh...|I could give a sh...|        156|c6aggux|I could give a sh...|               apple|    t5_2qh1f|I don't drive lik...|         18|    null|\n",
      "|          chrom_ed|So you're saying ...|So you're saying ...|        134|c6agxtv|So you're saying ...|               apple|    t5_2qh1f|you don't seem to...|          9|    null|\n",
      "|      gadzookfilms|I love this idea ...|I love this idea ...|        126|c6asb7p|I love this idea ...|RedditFilmsProduc...|    t5_2v33h|How we make money...|          9|    null|\n",
      "|      iamacannibal|Theres an entire ...|Theres an entire ...|        181|c6aveyw|Theres an entire ...|       AbandonedPorn|    t5_2sh6t|I'll try and get ...|         25|    null|\n",
      "| splagaticusxoxo97|FALSE. Evidence: ...|FALSE. Evidence: ...|        124|c6bacqq|FALSE. Evidence: ...|             atheism|    t5_2qh2p|dont fuck with re...|          6|    null|\n",
      "|           orthzar|If the number of ...|If the number of ...|         12|c6b83kp|If the number of ...|              quotes|    t5_2qhdx|                  no|          1|    null|\n",
      "|          phyzishy|Yeah, but most fo...|Yeah, but most fo...|         75|c6b52m8|Yeah, but most fo...|           AskReddit|    t5_2qh1i|       stupid stuff.|          2|    null|\n",
      "|          Wheelman|As an entrepreneu...|As an entrepreneu...|         78|c6b34c2|As an entrepreneu...|     personalfinance|    t5_2qstm|get a good CPA - ...|         14|    null|\n",
      "|        slagahthor|i guess the way I...|i guess the way I...|        323|c6b9gqo|i guess the way I...|             Animals|    t5_2qi0c|Dog neglected for...|          7|    null|\n",
      "|        Perservere|Didn't they lose ...|Didn't they lose ...|         86|c6bftvc|Didn't they lose ...|     leagueoflegends|    t5_2rfxx|just because you'...|         23|    null|\n",
      "|       fallsuspect|You probably won'...|You probably won'...|         79|c6bncqn|You probably won'...|           AskReddit|    t5_2qh1i|just get both of ...|         11|    null|\n",
      "|          captain0|To simply say tha...|To simply say tha...|        328|c6btcx4|To simply say tha...|              videos|    t5_2qh1e| Oppan Gangnam Style|          3|    null|\n",
      "|    Buck_Speedjunk|This picture does...|This picture does...|         18|c6c4uks|This picture does...|               trees|    t5_2r9vp|It's a half-assed...|         13|    null|\n",
      "|        FrankManic|And that is, hand...|And that is, hand...|         57|c6c7pgn|And that is, hand...|               Games|    t5_2qhwp|Play balance is f...|         13|    null|\n",
      "+------------------+--------------------+--------------------+-----------+-------+--------------------+--------------------+------------+--------------------+-----------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "105.45825719833374\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "hdfs_path = \"hdfs://grouop2master:9000//user/ubuntu/corpus-webis-tldr-17.json\" \n",
    "df = spark.read.json(hdfs_path)\n",
    "df.show()\n",
    "print(f\"{time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5505b53-3004-4941-8601-92c0fa1874be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                body|           subreddit|\n",
      "+--------------------+--------------------+\n",
      "|I think it should...|                math|\n",
      "|Art is about the ...|               funny|\n",
      "|Ask me what I thi...|         Borderlands|\n",
      "|In Mechwarrior On...|            gamingpc|\n",
      "|You are talking a...|              Diablo|\n",
      "|All but one of my...|   RedditLaqueristas|\n",
      "|I could give a sh...|               apple|\n",
      "|So you're saying ...|               apple|\n",
      "|I love this idea ...|RedditFilmsProduc...|\n",
      "|Theres an entire ...|       AbandonedPorn|\n",
      "|FALSE. Evidence: ...|             atheism|\n",
      "|If the number of ...|              quotes|\n",
      "|Yeah, but most fo...|           AskReddit|\n",
      "|As an entrepreneu...|     personalfinance|\n",
      "|i guess the way I...|             Animals|\n",
      "|Didn't they lose ...|     leagueoflegends|\n",
      "|You probably won'...|           AskReddit|\n",
      "|To simply say tha...|              videos|\n",
      "|This picture does...|               trees|\n",
      "|And that is, hand...|               Games|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.select(\"body\", \"subreddit\")  \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae1ebf94-38ed-4f9c-9e7b-06d66dd8f353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                body|           subreddit|\n",
      "+--------------------+--------------------+\n",
      "|I think it should...|                math|\n",
      "|Art is about the ...|               funny|\n",
      "|Ask me what I thi...|         borderlands|\n",
      "|In Mechwarrior On...|            gamingpc|\n",
      "|You are talking a...|              diablo|\n",
      "|All but one of my...|   redditlaqueristas|\n",
      "|I could give a sh...|               apple|\n",
      "|So you're saying ...|               apple|\n",
      "|I love this idea ...|redditfilmsproduc...|\n",
      "|Theres an entire ...|       abandonedporn|\n",
      "|FALSE. Evidence: ...|             atheism|\n",
      "|If the number of ...|              quotes|\n",
      "|Yeah, but most fo...|           askreddit|\n",
      "|As an entrepreneu...|     personalfinance|\n",
      "|i guess the way I...|             animals|\n",
      "|Didn't they lose ...|     leagueoflegends|\n",
      "|You probably won'...|           askreddit|\n",
      "|To simply say tha...|              videos|\n",
      "|This picture does...|               trees|\n",
      "|And that is, hand...|               games|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"subreddit\", lower(df[\"subreddit\"]))# convert subreddit categories to lowercase\n",
    "df = df.withColumn(\"subreddit\", trim(\"subreddit\"))# remove white spaces in subreddit\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2b6c335-c647-471f-a074-df137741c93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                body|           subreddit|\n",
      "+--------------------+--------------------+\n",
      "|I think it should...|                math|\n",
      "|Art is about the ...|               funny|\n",
      "|Ask me what I thi...|         borderlands|\n",
      "|In Mechwarrior On...|            gamingpc|\n",
      "|You are talking a...|              diablo|\n",
      "|All but one of my...|   redditlaqueristas|\n",
      "|I could give a sh...|               apple|\n",
      "|So youre saying t...|               apple|\n",
      "|I love this idea ...|redditfilmsproduc...|\n",
      "|Theres an entire ...|       abandonedporn|\n",
      "|FALSE Evidence Wo...|             atheism|\n",
      "|If the number of ...|              quotes|\n",
      "|Yeah but most fol...|           askreddit|\n",
      "|As an entrepreneu...|     personalfinance|\n",
      "|i guess the way I...|             animals|\n",
      "|Didnt they lose  ...|     leagueoflegends|\n",
      "|You probably wont...|           askreddit|\n",
      "|To simply say tha...|              videos|\n",
      "|This picture does...|               trees|\n",
      "|And that is hands...|               games|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('body', translate('body', '1234567890!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_{|}~', ' '))# remove punctuation and numbers in the body column\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68efc7f2-6c5d-4179-962d-f9c88ea5a7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                body|           subreddit|\n",
      "+--------------------+--------------------+\n",
      "|I think it should...|                math|\n",
      "|Art is about the ...|               funny|\n",
      "|Ask me what I thi...|         borderlands|\n",
      "|In Mechwarrior On...|            gamingpc|\n",
      "|You are talking a...|              diablo|\n",
      "|All but one of my...|   redditlaqueristas|\n",
      "|I could give a sh...|               apple|\n",
      "|So youre saying t...|               apple|\n",
      "|I love this idea ...|redditfilmsproduc...|\n",
      "|Theres an entire ...|       abandonedporn|\n",
      "|FALSE Evidence Wo...|             atheism|\n",
      "|If the number of ...|              quotes|\n",
      "|Yeah but most fol...|           askreddit|\n",
      "|As an entrepreneu...|     personalfinance|\n",
      "|i guess the way I...|             animals|\n",
      "|Didnt they lose g...|     leagueoflegends|\n",
      "|You probably wont...|           askreddit|\n",
      "|To simply say tha...|              videos|\n",
      "|This picture does...|               trees|\n",
      "|And that is hands...|               games|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"body\", regexp_replace(col(\"body\"), \"\\\\s+\", \" \"))#remove white spaces in body\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dfe09bb-18a7-4cee-ad30-ddb83ebbf7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                body|           subreddit|\n",
      "+--------------------+--------------------+\n",
      "|I think it should...|                math|\n",
      "|Art is about the ...|               funny|\n",
      "|Ask me what I thi...|         borderlands|\n",
      "|In Mechwarrior On...|            gamingpc|\n",
      "|You are talking a...|              diablo|\n",
      "|All but one of my...|   redditlaqueristas|\n",
      "|I could give a sh...|               apple|\n",
      "|So youre saying t...|               apple|\n",
      "|I love this idea ...|redditfilmsproduc...|\n",
      "|Theres an entire ...|       abandonedporn|\n",
      "|FALSE Evidence Wo...|             atheism|\n",
      "|If the number of ...|              quotes|\n",
      "|Yeah but most fol...|           askreddit|\n",
      "|As an entrepreneu...|     personalfinance|\n",
      "|i guess the way I...|             animals|\n",
      "|Didnt they lose g...|     leagueoflegends|\n",
      "|You probably wont...|           askreddit|\n",
      "|To simply say tha...|              videos|\n",
      "|This picture does...|               trees|\n",
      "|And that is hands...|               games|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.filter(df.body != '')# delete empty lines\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae4cd491-03e9-47cd-ad98-2339a2862287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                body|           subreddit|               words|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|I think it should...|                math|[i, think, it, sh...|\n",
      "|Art is about the ...|               funny|[art, is, about, ...|\n",
      "|Ask me what I thi...|         borderlands|[ask, me, what, i...|\n",
      "|In Mechwarrior On...|            gamingpc|[in, mechwarrior,...|\n",
      "|You are talking a...|              diablo|[you, are, talkin...|\n",
      "|All but one of my...|   redditlaqueristas|[all, but, one, o...|\n",
      "|I could give a sh...|               apple|[i, could, give, ...|\n",
      "|So youre saying t...|               apple|[so, youre, sayin...|\n",
      "|I love this idea ...|redditfilmsproduc...|[i, love, this, i...|\n",
      "|Theres an entire ...|       abandonedporn|[theres, an, enti...|\n",
      "|FALSE Evidence Wo...|             atheism|[false, evidence,...|\n",
      "|If the number of ...|              quotes|[if, the, number,...|\n",
      "|Yeah but most fol...|           askreddit|[yeah, but, most,...|\n",
      "|As an entrepreneu...|     personalfinance|[as, an, entrepre...|\n",
      "|i guess the way I...|             animals|[i, guess, the, w...|\n",
      "|Didnt they lose g...|     leagueoflegends|[didnt, they, los...|\n",
      "|You probably wont...|           askreddit|[you, probably, w...|\n",
      "|To simply say tha...|              videos|[to, simply, say,...|\n",
      "|This picture does...|               trees|[this, picture, d...|\n",
      "|And that is hands...|               games|[and, that, is, h...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"body\", outputCol=\"words\")\n",
    "df = tokenizer.transform(df)# split each sentence into separate words\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c22f3678-d9c0-4705-8e87-5d123cf65516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|                body|           subreddit|               words|      filtered_words|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|I think it should...|                math|[i, think, it, sh...|[think, fixed, ei...|\n",
      "|Art is about the ...|               funny|[art, is, about, ...|[art, hardest, th...|\n",
      "|Ask me what I thi...|         borderlands|[ask, me, what, i...|[ask, think, wall...|\n",
      "|In Mechwarrior On...|            gamingpc|[in, mechwarrior,...|[mechwarrior, onl...|\n",
      "|You are talking a...|              diablo|[you, are, talkin...|[talking, charsi,...|\n",
      "|All but one of my...|   redditlaqueristas|[all, but, one, o...|[one, nails, ball...|\n",
      "|I could give a sh...|               apple|[i, could, give, ...|[give, shit, turn...|\n",
      "|So youre saying t...|               apple|[so, youre, sayin...|[youre, saying, t...|\n",
      "|I love this idea ...|redditfilmsproduc...|[i, love, this, i...|[love, idea, defi...|\n",
      "|Theres an entire ...|       abandonedporn|[theres, an, enti...|[theres, entire, ...|\n",
      "|FALSE Evidence Wo...|             atheism|[false, evidence,...|[false, evidence,...|\n",
      "|If the number of ...|              quotes|[if, the, number,...|[number, sides, c...|\n",
      "|Yeah but most fol...|           askreddit|[yeah, but, most,...|[yeah, folks, thi...|\n",
      "|As an entrepreneu...|     personalfinance|[as, an, entrepre...|[entrepreneurfree...|\n",
      "|i guess the way I...|             animals|[i, guess, the, w...|[guess, way, tell...|\n",
      "|Didnt they lose g...|     leagueoflegends|[didnt, they, los...|[didnt, lose, gam...|\n",
      "|You probably wont...|           askreddit|[you, probably, w...|[probably, wont, ...|\n",
      "|To simply say tha...|              videos|[to, simply, say,...|[simply, say, mas...|\n",
      "|This picture does...|               trees|[this, picture, d...|[picture, doesnt,...|\n",
      "|And that is hands...|               games|[and, that, is, h...|[hands, coolest, ...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "df = stopwords_remover.transform(df) # remove stopwords\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80ff884a-3444-42bc-9316-4a74cb9f1b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+----------------+--------------+\n",
      "|           subreddit|total_word_count|other_word_count|sentence_count|\n",
      "+--------------------+----------------+----------------+--------------+\n",
      "|               anime|         1565594|          806284|          5868|\n",
      "|             shotbow|           38231|           19344|           153|\n",
      "|                tmnt|           20841|           10652|            54|\n",
      "|       crohnsdisease|          193440|           95665|           594|\n",
      "|        marvelheroes|          102752|           53774|           416|\n",
      "|                 art|          160596|           79984|           688|\n",
      "|              travel|          885554|          447319|          3076|\n",
      "|             4runner|            6416|            3331|            26|\n",
      "|            lacrosse|           45306|           22968|           246|\n",
      "|              poetry|           64591|           32451|           220|\n",
      "|      dippingtobacco|           53285|           26489|           289|\n",
      "|londonfootballmeetup|            1802|             888|             6|\n",
      "|           kitchener|            6444|            3196|            27|\n",
      "|                cubs|           21827|           11318|            85|\n",
      "|          costa_rica|            9621|            4871|            31|\n",
      "|               mcnsa|            8273|            4234|            39|\n",
      "|        couchsurfing|           21967|           10752|            68|\n",
      "|     youtubecomments|            1303|             698|             8|\n",
      "|           metro2033|            1967|            1068|            16|\n",
      "|             marxism|            1822|             958|            10|\n",
      "+--------------------+----------------+----------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "384.1956925392151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "result = df.groupBy(\"subreddit\").agg(\n",
    "    F.sum(F.size(\"words\")).alias(\"total_word_count\"),#calculate the number of all words in each subreddit category\n",
    "    F.sum(F.size(\"filtered_words\")).alias(\"other_word_count\"),#calculate the number of words after removing stop words in each subreddit category\n",
    "    F.count(\"body\").alias(\"sentence_count\")#calculate the number of requests in each subreddit category\n",
    ")\n",
    "\n",
    "result.show()\n",
    "print(f\"{time.time() - start_time}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e94af612-f202-4350-9254-1ef341216344",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------+----------------+--------------+\n",
      "|subreddit          |total_word_count|other_word_count|sentence_count|\n",
      "+-------------------+----------------+----------------+--------------+\n",
      "|askreddit          |145463239       |70295937        |589947        |\n",
      "|relationships      |173638495       |78013133        |352049        |\n",
      "|leagueoflegends    |25763882        |13366879        |109307        |\n",
      "|tifu               |19268244        |9257272         |52219         |\n",
      "|relationship_advice|24645327        |11050084        |50416         |\n",
      "|trees              |11815990        |5747224         |47286         |\n",
      "|gaming             |9176368         |4696013         |43851         |\n",
      "|atheism            |11621663        |5666301         |43268         |\n",
      "|adviceanimals      |7872130         |3883801         |40783         |\n",
      "|funny              |6947389         |3497120         |40171         |\n",
      "|politics           |8614353         |4429484         |36518         |\n",
      "|pics               |6011709         |3032386         |35098         |\n",
      "|sex                |9072066         |4231503         |28806         |\n",
      "|wtf                |4713252         |2363365         |25781         |\n",
      "|explainlikeimfive  |6169612         |3186121         |25482         |\n",
      "|todayilearned      |4767997         |2456907         |25004         |\n",
      "|fitness            |5236232         |2706154         |22694         |\n",
      "|iama               |5548844         |2738448         |22689         |\n",
      "|worldnews          |4925650         |2549791         |22577         |\n",
      "|dota2              |5363116         |2817158         |22405         |\n",
      "+-------------------+----------------+----------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "362.5741994380951\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "popular_subreddit= result.orderBy(col(\"sentence_count\").desc())# find the most popular category\n",
    "popular_subreddit.show(truncate=False)\n",
    "print(f\"{time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2635e636-802a-4739-8448-79a404ae86f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#most_popular_subredits = result.filter(result.sentence_count > 10)# display the category, where the percentage of stopwords is the highest and the number of requests is more than 10\n",
    "#most_popular_subredits.orderBy(col(\"stop_word_percentage\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ffe0f06-e68c-4904-b931-f5ddd919a84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             ngrams2|\n",
      "+--------------------+\n",
      "|[i think, think i...|\n",
      "|[art is, is about...|\n",
      "|[ask me, me what,...|\n",
      "|[in mechwarrior, ...|\n",
      "|[you are, are tal...|\n",
      "|[all but, but one...|\n",
      "|[i could, could g...|\n",
      "|[so youre, youre ...|\n",
      "|[i love, love thi...|\n",
      "|[theres an, an en...|\n",
      "|[false evidence, ...|\n",
      "|[if the, the numb...|\n",
      "|[yeah but, but mo...|\n",
      "|[as an, an entrep...|\n",
      "|[i guess, guess t...|\n",
      "|[didnt they, they...|\n",
      "|[you probably, pr...|\n",
      "|[to simply, simpl...|\n",
      "|[this picture, pi...|\n",
      "|[and that, that i...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "3.1085190773010254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams2\")# ngram calculation where n=2\n",
    "df = ngram.transform(df)\n",
    "df.select(\"ngrams2\").show()\n",
    "print(f\"{time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a13cf68e-4d17-44f3-a9fe-85f5cac94729",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(pair2='of the', count=3478162)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|  pair2|  count|\n",
      "+-------+-------+\n",
      "| of the|3478162|\n",
      "| in the|3389036|\n",
      "|  and i|2871662|\n",
      "|  i was|2865442|\n",
      "|  to be|2200242|\n",
      "| to the|2013867|\n",
      "| i have|1898029|\n",
      "| on the|1744995|\n",
      "| that i|1665249|\n",
      "| i dont|1658424|\n",
      "|  but i|1568086|\n",
      "| it was|1517470|\n",
      "|want to|1502303|\n",
      "|   i am|1461029|\n",
      "|for the|1420812|\n",
      "|   in a|1403413|\n",
      "|  for a|1313219|\n",
      "|   so i|1284937|\n",
      "| to get|1263131|\n",
      "| if you|1158753|\n",
      "+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "969.388587474823\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "all_pairs_df = df.select(F.explode(\"ngrams2\").alias(\"pair2\"))# transform each pair into a separate row\n",
    "pair_counts = all_pairs_df.groupBy(\"pair2\").count()# count the number of pairs\n",
    "most_common_word_pair = pair_counts.orderBy(F.desc(\"count\")).first()\n",
    "print(most_common_word_pair)# print the most popular pair\n",
    "pair_counts.orderBy(F.desc(\"count\")).show()# print first 20 popular pairs\n",
    "print(f\"{time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "edbf4405-8de5-4408-a272-b9bb36449edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|              ngrams|\n",
      "+--------------------+\n",
      "|[i think it, thin...|\n",
      "|[art is about, is...|\n",
      "|[ask me what, me ...|\n",
      "|[in mechwarrior o...|\n",
      "|[you are talking,...|\n",
      "|[all but one, but...|\n",
      "|[i could give, co...|\n",
      "|[so youre saying,...|\n",
      "|[i love this, lov...|\n",
      "|[theres an entire...|\n",
      "|[false evidence w...|\n",
      "|[if the number, t...|\n",
      "|[yeah but most, b...|\n",
      "|[as an entreprene...|\n",
      "|[i guess the, gue...|\n",
      "|[didnt they lose,...|\n",
      "|[you probably won...|\n",
      "|[to simply say, s...|\n",
      "|[this picture doe...|\n",
      "|[and that is, tha...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ngram = NGram(n=3, inputCol=\"words\", outputCol=\"ngrams\")# ngram calculation where n=3\n",
    "df = ngram.transform(df)\n",
    "df.select(\"ngrams\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "529ff472-dc26-4ed8-bb27-e06d907ed8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/15 09:43:42 ERROR TaskSchedulerImpl: Lost executor 0 on 192.168.2.226: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "24/03/15 09:43:42 WARN TaskSetManager: Lost task 15.0 in stage 20.0 (TID 1508, 192.168.2.226, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "24/03/15 09:43:42 WARN TaskSetManager: Lost task 14.0 in stage 20.0 (TID 1507, 192.168.2.226, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "24/03/15 09:43:47 WARN TaskSetManager: Lost task 33.0 in stage 20.0 (TID 1526, 192.168.2.248, executor 2): FetchFailed(BlockManagerId(0, 192.168.2.226, 34209, None), shuffleId=4, mapIndex=2, mapId=1348, reduceId=33, message=\n",
      "org.apache.spark.shuffle.FetchFailedException: The relative remote executor(Id: 0), which maintains the block data to fetch is dead.\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:748)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:663)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n",
      "\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:31)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:628)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1492)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:837)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:837)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.ExecutorDeadException: The relative remote executor(Id: 0), which maintains the block data to fetch is dead.\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:132)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:169)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\t... 1 more\n",
      "\n",
      ")\n",
      "24/03/15 09:43:47 WARN TaskSetManager: Lost task 32.0 in stage 20.0 (TID 1525, 192.168.2.248, executor 2): FetchFailed(BlockManagerId(0, 192.168.2.226, 34209, None), shuffleId=4, mapIndex=78, mapId=1424, reduceId=32, message=\n",
      "org.apache.spark.shuffle.FetchFailedException: The relative remote executor(Id: 0), which maintains the block data to fetch is dead.\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:748)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:663)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n",
      "\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:31)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:628)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1492)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:837)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:837)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.ExecutorDeadException: The relative remote executor(Id: 0), which maintains the block data to fetch is dead.\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:132)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:169)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\t... 1 more\n",
      "\n",
      ")\n",
      "24/03/15 09:43:47 WARN TaskSetManager: Lost task 14.1 in stage 20.0 (TID 1529, 192.168.2.248, executor 2): FetchFailed(BlockManagerId(0, 192.168.2.226, 34209, None), shuffleId=4, mapIndex=2, mapId=1348, reduceId=14, message=\n",
      "org.apache.spark.shuffle.FetchFailedException: The relative remote executor(Id: 0), which maintains the block data to fetch is dead.\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:748)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:663)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n",
      "\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:31)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:628)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1492)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:837)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:837)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.ExecutorDeadException: The relative remote executor(Id: 0), which maintains the block data to fetch is dead.\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:132)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.start(RetryingBlockFetcher.java:121)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:143)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.sendRequest(ShuffleBlockFetcherIterator.scala:278)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.send$1(ShuffleBlockFetcherIterator.scala:721)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchUpToMaxBytes(ShuffleBlockFetcherIterator.scala:716)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:530)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:171)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:83)\n",
      "\tat org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:207)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\t... 11 more\n",
      "\n",
      ")\n",
      "24/03/15 09:43:47 WARN TaskSetManager: Lost task 34.0 in stage 20.0 (TID 1527, 192.168.2.247, executor 1): FetchFailed(BlockManagerId(0, 192.168.2.226, 34209, None), shuffleId=4, mapIndex=78, mapId=1424, reduceId=34, message=\n",
      "org.apache.spark.shuffle.FetchFailedException: The relative remote executor(Id: 0), which maintains the block data to fetch is dead.\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:748)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:663)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n",
      "\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:31)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:628)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1492)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:837)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:837)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.ExecutorDeadException: The relative remote executor(Id: 0), which maintains the block data to fetch is dead.\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:132)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:169)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\t... 1 more\n",
      "\n",
      ")\n",
      "24/03/15 09:43:47 WARN TaskSetManager: Lost task 35.0 in stage 20.0 (TID 1528, 192.168.2.247, executor 1): FetchFailed(BlockManagerId(0, 192.168.2.226, 34209, None), shuffleId=4, mapIndex=2, mapId=1348, reduceId=35, message=\n",
      "org.apache.spark.shuffle.FetchFailedException: The relative remote executor(Id: 0), which maintains the block data to fetch is dead.\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:748)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:663)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n",
      "\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:31)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:628)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1492)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:837)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:837)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.ExecutorDeadException: The relative remote executor(Id: 0), which maintains the block data to fetch is dead.\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:132)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:169)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\t... 1 more\n",
      "\n",
      ")\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(pair='a lot of', count=686485)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+\n",
      "|         pair| count|\n",
      "+-------------+------+\n",
      "|     a lot of|686485|\n",
      "|  i dont know|399654|\n",
      "|    i want to|379181|\n",
      "|   one of the|313803|\n",
      "|  i feel like|280892|\n",
      "| dont want to|274700|\n",
      "|   be able to|265794|\n",
      "|     i have a|262682|\n",
      "|  i dont want|243911|\n",
      "|   out of the|237501|\n",
      "|   when i was|227219|\n",
      "|the fact that|214354|\n",
      "|      to be a|213047|\n",
      "|at this point|200203|\n",
      "|     it was a|192219|\n",
      "|  i have been|192081|\n",
      "|   and i have|183191|\n",
      "|   the end of|181640|\n",
      "|  going to be|177218|\n",
      "|   what to do|175644|\n",
      "+-------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "1970.7725327014923\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "all_pairs_df = df.select(F.explode(\"ngrams\").alias(\"pair\"))# transform each pair into a separate row\n",
    "pair_counts = all_pairs_df.groupBy(\"pair\").count()# count the number of pairs\n",
    "most_common_word_pair = pair_counts.orderBy(F.desc(\"count\")).first()\n",
    "print(most_common_word_pair)# print the most popular pair\n",
    "pair_counts.orderBy(F.desc(\"count\")).show()# print first 20 popular pairs\n",
    "print(f\"{time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd2f361-53b4-45bb-9de8-81a9f209c15b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.8",
   "language": "python",
   "name": "python3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
